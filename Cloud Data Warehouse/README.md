# Cloud Data Warehouse for Gaming Company

A gaming company sought to gain deeper insights into their players by harnessing the data stored in their Amazon S3 data lakes. Their goal was to convert this data into a structured format and ingest it into a cloud data warehouse, enabling low query response times. To achieve this, they opted to utilize AWS Glue for data conversion and Amazon Redshift as their cloud data warehouse, leveraging Amazon Redshift Spectrum to query data in Amazon S3 without needing to move it.

# Lab Architecture

![0  Picture 0](https://github.com/kevin-wynn-cloud/AWS-Projects/assets/144941082/ebf08dd3-e9c2-4dfe-9f11-4460d49a5a39)

# Step 1: Creating Glue Script

In the initial step, I accessed AWS Glue Studio and selected the Spark script editor. I proceeded to create a new script with boilerplate code. The GlueContext class was employed to interact with the Apache Spark framework. The script performed the following operations:

- Importing necessary modules.
- Initializing SparkContext, GlueContext, and the job.
- Reading data from a cataloged table in the "games-data-db" database.
- Relationalizing the data to flatten nested structures.
- Selecting and grouping data.
- Writing flattened data to the S3 consumption data bucket in Parquet format.

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

glueContext = GlueContext(SparkContext.getOrCreate())

df = glueContext.create_dynamic_frame.from_catalog(database="games-data-db", table_name="raw_data_714823731171_440")


## The built-in transformation 'relationalize' is used to flatten the nested data structures. 
## This transofromation relationalizes a DynamicFrame and produces a collection of DynamicFrames 
## that are generated by unnesting nested columns and pivoting array columns. 
dfc = df.relationalize("root", "s3://consumption-data-714823731171-440")

## In this case, two tables 'root' and 'root_game_details' are generated.
flatdf = dfc.select('root')
flatdf2 = dfc.select('root_game_details')

## Group all the partitions into one file
flatdf_group = flatdf.coalesce(1)

## In the root_game_details DynamicFrame, rename column names that have dots as separator to names without dots. 
flatdf2_reformat_1 = RenameField.apply(flatdf2,"`game_details.val.game_name`", "game_name")
flatdf2_reformat_2 = RenameField.apply(flatdf2_reformat_1,"`game_details.val.high_score`", "high_score")
flatdf2_reformat_3 = RenameField.apply(flatdf2_reformat_2,"`game_details.val.purchased_item`", "purchased_item")
flatdf2_reformat_4 = RenameField.apply(flatdf2_reformat_3,"`game_details.val.purchases`", "purchases")

## Group all the partitions into one file
flatdf2_group = flatdf2_reformat_4.coalesce(1)

## Write the two flatten tables into the S3 consumption data bucket in parquet format.
glueContext.write_dynamic_frame.from_options(flatdf,connection_type="s3",connection_options = {"path":"s3://consumption-data-714823731171-440/parquet/players_data/"}, format = "parquet")
glueContext.write_dynamic_frame.from_options(flatdf2_reformat_4,connection_type="s3",connection_options = {"path":"s3://consumption-data-714823731171-440/parquet/games_data/"}, format = "parquet")

job.commit()
```

# Step 2: Running Glue Job

In this step, I went to Job Details and named the job "games-flattened-data-job." I specified the required AWS Glue role and executed the job.

![2  Picture 2](https://github.com/kevin-wynn-cloud/AWS-Projects/assets/144941082/c0428f55-eeb2-4384-bb1e-5d785f3ab411)

# Step 3: Creating a Crawler

Next, I created a crawler named "games-flattened-data-crawler." I added the S3 consumption bucket as the data source, selected the path to the Parquet directory within the bucket, and utilized the "aws_glue_role" for necessary resource access. The crawler targeted the "games-flattened-data-db" database and was executed.

![3  Picture 3](https://github.com/kevin-wynn-cloud/AWS-Projects/assets/144941082/84793747-390a-44fc-9ae8-7d06cb15a6bf)

# Step 4: Setting Up Amazon Redshift

After configuring AWS Glue, I moved on to Amazon Redshift. I managed the cluster and used the IAM role ARN to connect to the "games_rn_db" database in the query editor. I executed the following SQL queries to create an external schema and a materialized view:

```sql
create EXTERNAL SCHEMA players_schema
FROM DATA CATALOG
database 'games-flattened-data-db'
IAM_ROLE 'arn:aws:iam::714843771171:role/LabStack-be2b4f0c-b60f-40-redshiftspectrumroleE019-1WL4XTWOXEUKC'
create EXTERNAL DATABASE if not exists;
```

```sql
DROP MATERIALIZED VIEW IF EXISTS mv_players_purchases;
CREATE MATERIALIZED VIEW mv_players_purchases AS (
SELECT g.id as Id, p.name as Name, g.purchased_item as Purchased_item, '$'||g.purchases as Purchases
FROM players_schema.players_data p, players_schema.games_data g
WHERE p.game_details = g.id); 
```

I then created another materialized view to complete the lab, this time for calculating the total purchases made by players.

```sql
DROP MATERIALIZED VIEW IF EXISTS mv_players_purchases_amount;
CREATE MATERIALIZED VIEW mv_players_purchases_amount AS (
    SELECT g.id as Id, p.name as Name, '$'||SUM(g.purchases) as Total_Purchases
    FROM players_schema.players_data p, players_schema.games_data g
    WHERE p.game_details = g.id 
    GROUP BY g.id, p.name
);
```
